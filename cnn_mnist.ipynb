{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T14:24:22.268133Z",
     "start_time": "2025-05-12T14:24:22.261037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_path = r\"D:\\python_project\\CNN-MNIST\"\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)\n",
    "\n",
    "os.chdir(project_path)\n",
    "print(os.getcwd())\n"
   ],
   "id": "431261f893fb557",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\python_project\\CNN-MNIST\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T14:24:25.053515Z",
     "start_time": "2025-05-12T14:24:25.023330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.data_utils import get_mnist_loaders\n",
    "\n",
    "train_loader, test_loader, device = get_mnist_loaders(\n",
    "    data_dir='./data/MNIST',  # 指向你手动放置的数据目录\n",
    "    batch_size=32,\n",
    "    use_cuda=False\n",
    ")\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "print(images.shape)  # torch.Size([32, 1, 28, 28])\n",
    "print(labels[:5])\n"
   ],
   "id": "f2b79a5f83f36be9",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dataset not found. You can use download=True to download it",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_mnist_loaders\n\u001B[1;32m----> 3\u001B[0m train_loader, test_loader, device \u001B[38;5;241m=\u001B[39m get_mnist_loaders(\n\u001B[0;32m      4\u001B[0m     data_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./data/MNIST\u001B[39m\u001B[38;5;124m'\u001B[39m,  \u001B[38;5;66;03m# 指向你手动放置的数据目录\u001B[39;00m\n\u001B[0;32m      5\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m,\n\u001B[0;32m      6\u001B[0m     use_cuda\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m      7\u001B[0m )\n\u001B[0;32m      9\u001B[0m images, labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28miter\u001B[39m(train_loader))\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(images\u001B[38;5;241m.\u001B[39mshape)  \u001B[38;5;66;03m# torch.Size([32, 1, 28, 28])\u001B[39;00m\n",
      "File \u001B[1;32mD:\\python_project\\CNN-MNIST\\utils\\data_utils.py:41\u001B[0m, in \u001B[0;36mget_mnist_loaders\u001B[1;34m(data_dir, batch_size, test_batch_size, seed, use_cuda, shuffle, num_workers, pin_memory)\u001B[0m\n\u001B[0;32m     35\u001B[0m transform \u001B[38;5;241m=\u001B[39m transforms\u001B[38;5;241m.\u001B[39mCompose([\n\u001B[0;32m     36\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mToTensor(),\n\u001B[0;32m     37\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mNormalize((\u001B[38;5;241m0.1307\u001B[39m,), (\u001B[38;5;241m0.3081\u001B[39m,))\n\u001B[0;32m     38\u001B[0m ])\n\u001B[0;32m     40\u001B[0m \u001B[38;5;66;03m# 设置 download=False，避免重新下载\u001B[39;00m\n\u001B[1;32m---> 41\u001B[0m dataset1 \u001B[38;5;241m=\u001B[39m datasets\u001B[38;5;241m.\u001B[39mMNIST(root\u001B[38;5;241m=\u001B[39mdata_dir, train\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, download\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, transform\u001B[38;5;241m=\u001B[39mtransform)\n\u001B[0;32m     42\u001B[0m dataset2 \u001B[38;5;241m=\u001B[39m datasets\u001B[38;5;241m.\u001B[39mMNIST(root\u001B[38;5;241m=\u001B[39mdata_dir, train\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, download\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, transform\u001B[38;5;241m=\u001B[39mtransform)\n\u001B[0;32m     44\u001B[0m train_loader \u001B[38;5;241m=\u001B[39m DataLoader(dataset1, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtrain_kwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\pytorch\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:103\u001B[0m, in \u001B[0;36mMNIST.__init__\u001B[1;34m(self, root, train, transform, target_transform, download)\u001B[0m\n\u001B[0;32m    100\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdownload()\n\u001B[0;32m    102\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_exists():\n\u001B[1;32m--> 103\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset not found. You can use download=True to download it\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    105\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtargets \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_load_data()\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Dataset not found. You can use download=True to download it"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(images[0][0], cmap='gray')\n",
    "plt.title(f\"Label: {labels[0].item()}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "id": "5114623b617bb1e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from model.CNN import CNNnet\n",
    "\n",
    "model = CNNnet().to(device)"
   ],
   "id": "1188055b4278df6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T14:24:47.652341Z",
     "start_time": "2025-05-12T14:24:47.612641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.hparams import get_hyperparams\n",
    "from model.CNN import CNNnet\n",
    "from model.train import train\n",
    "from model.test import test\n",
    "from utils.data_utils import get_mnist_loaders\n",
    "import torch.optim as optim\n",
    "\n",
    "# 加载所有超参数\n",
    "args = get_hyperparams()\n",
    "\n",
    "# 数据加载器\n",
    "train_loader, test_loader, device = get_mnist_loaders(\n",
    "    data_dir='./data/MNIST/MNIST',\n",
    "    batch_size=args.batch_size,\n",
    "    test_batch_size=args.test_batch_size,\n",
    "    seed=args.seed,\n",
    "    use_cuda=args.use_cuda,\n",
    "    shuffle=args.shuffle,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=args.pin_memory\n",
    ")\n",
    "\n",
    "# 初始化模型与优化器\n",
    "model = CNNnet().to(device)\n",
    "optimizer_map = {\n",
    "    'Adam': optim.Adam,\n",
    "    'AdaGrad': optim.Adagrad,\n",
    "    'RMSProp': optim.RMSprop\n",
    "}\n",
    "optimizer_cls = optimizer_map[args.optimizer]\n",
    "optimizer = optimizer_cls(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "# 训练 & 测试\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch, log_interval=args.log_interval)\n",
    "    test(model, device, test_loader)\n"
   ],
   "id": "525ac80daedbd108",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dataset not found. You can use download=True to download it",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 12\u001B[0m\n\u001B[0;32m      9\u001B[0m args \u001B[38;5;241m=\u001B[39m get_hyperparams()\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# 数据加载器\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m train_loader, test_loader, device \u001B[38;5;241m=\u001B[39m get_mnist_loaders(\n\u001B[0;32m     13\u001B[0m     data_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./data/MNIST/MNIST\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     14\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39margs\u001B[38;5;241m.\u001B[39mbatch_size,\n\u001B[0;32m     15\u001B[0m     test_batch_size\u001B[38;5;241m=\u001B[39margs\u001B[38;5;241m.\u001B[39mtest_batch_size,\n\u001B[0;32m     16\u001B[0m     seed\u001B[38;5;241m=\u001B[39margs\u001B[38;5;241m.\u001B[39mseed,\n\u001B[0;32m     17\u001B[0m     use_cuda\u001B[38;5;241m=\u001B[39margs\u001B[38;5;241m.\u001B[39muse_cuda,\n\u001B[0;32m     18\u001B[0m     shuffle\u001B[38;5;241m=\u001B[39margs\u001B[38;5;241m.\u001B[39mshuffle,\n\u001B[0;32m     19\u001B[0m     num_workers\u001B[38;5;241m=\u001B[39margs\u001B[38;5;241m.\u001B[39mnum_workers,\n\u001B[0;32m     20\u001B[0m     pin_memory\u001B[38;5;241m=\u001B[39margs\u001B[38;5;241m.\u001B[39mpin_memory\n\u001B[0;32m     21\u001B[0m )\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# 初始化模型与优化器\u001B[39;00m\n\u001B[0;32m     24\u001B[0m model \u001B[38;5;241m=\u001B[39m CNNnet()\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[1;32mD:\\python_project\\CNN-MNIST\\utils\\data_utils.py:41\u001B[0m, in \u001B[0;36mget_mnist_loaders\u001B[1;34m(data_dir, batch_size, test_batch_size, seed, use_cuda, shuffle, num_workers, pin_memory)\u001B[0m\n\u001B[0;32m     35\u001B[0m transform \u001B[38;5;241m=\u001B[39m transforms\u001B[38;5;241m.\u001B[39mCompose([\n\u001B[0;32m     36\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mToTensor(),\n\u001B[0;32m     37\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mNormalize((\u001B[38;5;241m0.1307\u001B[39m,), (\u001B[38;5;241m0.3081\u001B[39m,))\n\u001B[0;32m     38\u001B[0m ])\n\u001B[0;32m     40\u001B[0m \u001B[38;5;66;03m# 设置 download=False，避免重新下载\u001B[39;00m\n\u001B[1;32m---> 41\u001B[0m dataset1 \u001B[38;5;241m=\u001B[39m datasets\u001B[38;5;241m.\u001B[39mMNIST(root\u001B[38;5;241m=\u001B[39mdata_dir, train\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, download\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, transform\u001B[38;5;241m=\u001B[39mtransform)\n\u001B[0;32m     42\u001B[0m dataset2 \u001B[38;5;241m=\u001B[39m datasets\u001B[38;5;241m.\u001B[39mMNIST(root\u001B[38;5;241m=\u001B[39mdata_dir, train\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, download\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, transform\u001B[38;5;241m=\u001B[39mtransform)\n\u001B[0;32m     44\u001B[0m train_loader \u001B[38;5;241m=\u001B[39m DataLoader(dataset1, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtrain_kwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\pytorch\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:103\u001B[0m, in \u001B[0;36mMNIST.__init__\u001B[1;34m(self, root, train, transform, target_transform, download)\u001B[0m\n\u001B[0;32m    100\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdownload()\n\u001B[0;32m    102\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_exists():\n\u001B[1;32m--> 103\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset not found. You can use download=True to download it\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    105\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtargets \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_load_data()\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Dataset not found. You can use download=True to download it"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T09:54:16.022408Z",
     "start_time": "2025-05-05T09:38:44.692749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from model.CNN import CNNnet\n",
    "from model.train import train\n",
    "from model.test import test\n",
    "from utils.data_utils import get_mnist_loaders  # 强制重新加载模块\n",
    "from utils.hparams import get_hyperparams\n",
    "\n",
    "# 优化器和学习率组合\n",
    "optimizers = ['Adam', 'AdaGrad', 'RMSProp']\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "\n",
    "# 创建结果文件夹\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "# 结果记录字典\n",
    "result_matrix = {}\n",
    "\n",
    "for opt_name in optimizers:\n",
    "    result_matrix[opt_name] = {}\n",
    "    for lr in learning_rates:\n",
    "        print(f\"\\n===== Running {opt_name} with LR={lr} =====\")\n",
    "        args = get_hyperparams(optimizer=opt_name, lr=lr)\n",
    "        train_loader, test_loader, device = get_mnist_loaders(\n",
    "            batch_size=args.batch_size,\n",
    "            test_batch_size=args.test_batch_size,\n",
    "            seed=args.seed,\n",
    "            use_cuda=args.use_cuda,\n",
    "            shuffle=args.shuffle,\n",
    "            num_workers=args.num_workers,\n",
    "            pin_memory=args.pin_memory\n",
    "        )\n",
    "\n",
    "        model = CNNnet().to(device)\n",
    "        optimizer_map = {\n",
    "            'Adam': optim.Adam,\n",
    "            'AdaGrad': optim.Adagrad,\n",
    "            'RMSProp': optim.RMSprop\n",
    "        }\n",
    "        optimizer = optimizer_map[opt_name](model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "        train_losses = []\n",
    "        test_accuracies = []\n",
    "\n",
    "        for epoch in range(1, args.epochs + 1):\n",
    "            loss_vals = train(model, device, train_loader, optimizer, epoch, args.log_interval)\n",
    "            train_losses.append(sum(loss_vals) / len(loss_vals))\n",
    "            _, acc = test(model, device, test_loader)\n",
    "            test_accuracies.append(acc)\n",
    "\n",
    "        # 保存结果\n",
    "        result_matrix[opt_name][lr] = {\n",
    "            'train_loss': train_losses,\n",
    "            'test_acc': test_accuracies\n",
    "        }\n",
    "\n",
    "        # === 保存当前实验的曲线图 ===\n",
    "        plt.figure(figsize=(10, 4))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(range(1, len(train_losses)+1), train_losses, marker='o')\n",
    "        plt.title(f\"Train Loss ({opt_name}, lr={lr})\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(range(1, len(test_accuracies)+1), test_accuracies, marker='x')\n",
    "        plt.title(f\"Test Accuracy ({opt_name}, lr={lr})\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy (%)\")\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        img_name = f\"results/{opt_name}_lr{lr}.png\".replace('.', '_')\n",
    "        plt.savefig(img_name)\n",
    "        plt.close()\n"
   ],
   "id": "29bf75b6883a40ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Running Adam with LR=0.1 =====\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301220\tGrad Norm: 1.8177\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.311620\tGrad Norm: 0.1538\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.295421\tGrad Norm: 0.0678\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.308861\tGrad Norm: 0.1372\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.324122\tGrad Norm: 0.1248\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.286736\tGrad Norm: 0.1616\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.307298\tGrad Norm: 0.1088\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.291949\tGrad Norm: 0.1351\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.314352\tGrad Norm: 0.1027\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 2.332623\tGrad Norm: 0.1488\n",
      "\n",
      "Test set: Average loss: 2.3076, Accuracy: 1010/10000 (10.10%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.290286\tGrad Norm: 0.0697\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 2.255116\tGrad Norm: 0.1320\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 2.284818\tGrad Norm: 0.0740\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 2.324384\tGrad Norm: 0.1379\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 2.302158\tGrad Norm: 0.1046\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 2.275331\tGrad Norm: 0.0773\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 2.370557\tGrad Norm: 0.1788\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 2.293150\tGrad Norm: 0.1565\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 2.325016\tGrad Norm: 0.1726\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 2.305295\tGrad Norm: 0.1060\n",
      "\n",
      "Test set: Average loss: 2.3119, Accuracy: 1010/10000 (10.10%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.282424\tGrad Norm: 0.1081\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 2.293217\tGrad Norm: 0.1042\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 2.319575\tGrad Norm: 0.1123\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 2.326521\tGrad Norm: 0.1300\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 2.309047\tGrad Norm: 0.0533\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 2.291614\tGrad Norm: 0.0814\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 2.311014\tGrad Norm: 0.1008\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 2.296074\tGrad Norm: 0.0633\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 2.294422\tGrad Norm: 0.0791\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 2.284596\tGrad Norm: 0.1181\n",
      "\n",
      "Test set: Average loss: 2.3161, Accuracy: 1032/10000 (10.32%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.337622\tGrad Norm: 0.1189\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 2.322762\tGrad Norm: 0.1408\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 2.314681\tGrad Norm: 0.1149\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 2.314112\tGrad Norm: 0.0947\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 2.348685\tGrad Norm: 0.1551\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 2.318681\tGrad Norm: 0.1043\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 2.357665\tGrad Norm: 0.1590\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 2.309516\tGrad Norm: 0.1813\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 2.326787\tGrad Norm: 0.1626\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 2.327020\tGrad Norm: 0.1066\n",
      "\n",
      "Test set: Average loss: 2.3069, Accuracy: 1028/10000 (10.28%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.288555\tGrad Norm: 0.1011\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 2.316836\tGrad Norm: 0.1138\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 2.300327\tGrad Norm: 0.0947\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 2.351207\tGrad Norm: 0.1520\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 2.300351\tGrad Norm: 0.1192\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 2.344510\tGrad Norm: 0.1434\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 2.312090\tGrad Norm: 0.1167\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 2.301322\tGrad Norm: 0.0800\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 2.289700\tGrad Norm: 0.0990\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 2.322994\tGrad Norm: 0.1579\n",
      "\n",
      "Test set: Average loss: 2.3110, Accuracy: 1028/10000 (10.28%)\n",
      "\n",
      "\n",
      "===== Running Adam with LR=0.01 =====\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301220\tGrad Norm: 1.8177\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.581413\tGrad Norm: 2.0643\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.512039\tGrad Norm: 1.9768\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.610458\tGrad Norm: 3.6387\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.216275\tGrad Norm: 1.1785\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.224082\tGrad Norm: 0.8754\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.242672\tGrad Norm: 1.4376\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.251852\tGrad Norm: 1.2547\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.241646\tGrad Norm: 1.6031\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.365647\tGrad Norm: 1.4658\n",
      "\n",
      "Test set: Average loss: 0.1342, Accuracy: 9593/10000 (95.93%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.509817\tGrad Norm: 1.4964\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.240613\tGrad Norm: 1.5809\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.230641\tGrad Norm: 1.0870\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.246811\tGrad Norm: 0.7624\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.130778\tGrad Norm: 0.7605\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.254215\tGrad Norm: 1.9554\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.376023\tGrad Norm: 1.8127\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.163299\tGrad Norm: 0.8330\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.172818\tGrad Norm: 1.3412\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.307920\tGrad Norm: 1.1882\n",
      "\n",
      "Test set: Average loss: 0.1062, Accuracy: 9682/10000 (96.82%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.374723\tGrad Norm: 1.3642\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.306768\tGrad Norm: 1.4719\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.241471\tGrad Norm: 1.2629\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.176862\tGrad Norm: 0.7953\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.277323\tGrad Norm: 1.9176\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.183944\tGrad Norm: 1.3900\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.222705\tGrad Norm: 0.9569\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.220132\tGrad Norm: 1.5440\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.346232\tGrad Norm: 2.0046\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.346176\tGrad Norm: 2.4984\n",
      "\n",
      "Test set: Average loss: 0.0991, Accuracy: 9713/10000 (97.13%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.169768\tGrad Norm: 0.7504\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.095891\tGrad Norm: 0.7855\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.143569\tGrad Norm: 0.7472\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.276235\tGrad Norm: 1.2923\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.226543\tGrad Norm: 1.4118\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.077800\tGrad Norm: 0.5922\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.264029\tGrad Norm: 3.4184\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.164012\tGrad Norm: 0.9154\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.179722\tGrad Norm: 0.9705\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.180411\tGrad Norm: 1.1940\n",
      "\n",
      "Test set: Average loss: 0.1100, Accuracy: 9673/10000 (96.73%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.150393\tGrad Norm: 1.7432\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.169397\tGrad Norm: 1.0193\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.159127\tGrad Norm: 0.9239\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.279860\tGrad Norm: 1.9588\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.730185\tGrad Norm: 4.2418\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.526491\tGrad Norm: 2.1202\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.322056\tGrad Norm: 1.7697\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.205853\tGrad Norm: 1.2440\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.133127\tGrad Norm: 1.7036\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.104232\tGrad Norm: 0.7053\n",
      "\n",
      "Test set: Average loss: 0.1072, Accuracy: 9687/10000 (96.87%)\n",
      "\n",
      "\n",
      "===== Running Adam with LR=0.001 =====\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301220\tGrad Norm: 1.8177\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.251880\tGrad Norm: 1.7635\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.126035\tGrad Norm: 1.6011\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.069431\tGrad Norm: 1.3599\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.155358\tGrad Norm: 1.7397\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.053546\tGrad Norm: 0.9452\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.088809\tGrad Norm: 1.0301\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.038087\tGrad Norm: 0.5228\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.170897\tGrad Norm: 2.0218\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.165038\tGrad Norm: 1.2652\n",
      "\n",
      "Test set: Average loss: 0.0459, Accuracy: 9846/10000 (98.46%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.220695\tGrad Norm: 1.8080\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.022416\tGrad Norm: 0.5974\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.087692\tGrad Norm: 1.2836\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.094403\tGrad Norm: 0.9736\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.028290\tGrad Norm: 0.5708\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.153719\tGrad Norm: 2.3374\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.076948\tGrad Norm: 1.4387\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.034993\tGrad Norm: 1.2327\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.087605\tGrad Norm: 1.3754\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.107817\tGrad Norm: 1.1482\n",
      "\n",
      "Test set: Average loss: 0.0365, Accuracy: 9886/10000 (98.86%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.044026\tGrad Norm: 0.9794\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.062242\tGrad Norm: 1.0485\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.049702\tGrad Norm: 0.8495\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.034386\tGrad Norm: 0.7142\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.058825\tGrad Norm: 1.1747\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.042808\tGrad Norm: 1.0466\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.056204\tGrad Norm: 0.8943\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.040741\tGrad Norm: 1.2198\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.131657\tGrad Norm: 1.7031\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.033044\tGrad Norm: 0.8084\n",
      "\n",
      "Test set: Average loss: 0.0340, Accuracy: 9898/10000 (98.98%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.065838\tGrad Norm: 1.5260\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.036937\tGrad Norm: 0.9490\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.006297\tGrad Norm: 0.2329\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.054383\tGrad Norm: 0.9987\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.066728\tGrad Norm: 1.0998\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.034990\tGrad Norm: 1.0246\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.002575\tGrad Norm: 0.0845\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.009439\tGrad Norm: 0.2558\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.088953\tGrad Norm: 1.6966\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.023503\tGrad Norm: 0.5232\n",
      "\n",
      "Test set: Average loss: 0.0284, Accuracy: 9904/10000 (99.04%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.028789\tGrad Norm: 0.5176\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.052320\tGrad Norm: 0.8537\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.050798\tGrad Norm: 0.9412\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.012646\tGrad Norm: 0.4890\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.080487\tGrad Norm: 1.1264\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.107821\tGrad Norm: 1.2611\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.084895\tGrad Norm: 1.1176\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.007195\tGrad Norm: 0.3282\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.003585\tGrad Norm: 0.1279\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.005878\tGrad Norm: 0.1853\n",
      "\n",
      "Test set: Average loss: 0.0359, Accuracy: 9885/10000 (98.85%)\n",
      "\n",
      "\n",
      "===== Running AdaGrad with LR=0.1 =====\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301220\tGrad Norm: 1.8177\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.287235\tGrad Norm: 0.5470\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.138134\tGrad Norm: 2.7118\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.463418\tGrad Norm: 2.6020\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.411156\tGrad Norm: 1.6384\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.526250\tGrad Norm: 3.4507\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.343953\tGrad Norm: 3.0730\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.501754\tGrad Norm: 3.1608\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.384543\tGrad Norm: 3.0686\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.360947\tGrad Norm: 3.4162\n",
      "\n",
      "Test set: Average loss: 0.1816, Accuracy: 9421/10000 (94.21%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.422000\tGrad Norm: 1.6267\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.190940\tGrad Norm: 0.8892\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.256512\tGrad Norm: 4.0990\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.315362\tGrad Norm: 1.9860\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.208794\tGrad Norm: 1.5099\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.419664\tGrad Norm: 2.1610\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.345905\tGrad Norm: 1.7148\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.204178\tGrad Norm: 2.2493\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.219884\tGrad Norm: 2.8471\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.556850\tGrad Norm: 3.1891\n",
      "\n",
      "Test set: Average loss: 0.1213, Accuracy: 9608/10000 (96.08%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.329111\tGrad Norm: 3.3678\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.292412\tGrad Norm: 1.8847\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.174244\tGrad Norm: 1.7536\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.225543\tGrad Norm: 1.2010\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.318584\tGrad Norm: 1.6088\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.214907\tGrad Norm: 2.1160\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.248006\tGrad Norm: 1.2544\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.254374\tGrad Norm: 2.4321\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.382408\tGrad Norm: 2.9783\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.274071\tGrad Norm: 2.1373\n",
      "\n",
      "Test set: Average loss: 0.1013, Accuracy: 9685/10000 (96.85%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.120023\tGrad Norm: 2.2063\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.188857\tGrad Norm: 1.5473\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.114669\tGrad Norm: 0.8420\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.284685\tGrad Norm: 2.0208\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.148203\tGrad Norm: 1.6571\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.058752\tGrad Norm: 0.8679\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.153891\tGrad Norm: 1.8324\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.053347\tGrad Norm: 0.6489\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.340179\tGrad Norm: 2.0344\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.282392\tGrad Norm: 1.4950\n",
      "\n",
      "Test set: Average loss: 0.1002, Accuracy: 9666/10000 (96.66%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.108440\tGrad Norm: 1.5891\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.202882\tGrad Norm: 2.6022\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.150815\tGrad Norm: 2.3231\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.207328\tGrad Norm: 2.0588\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.242204\tGrad Norm: 1.6228\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.285715\tGrad Norm: 2.5444\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.215125\tGrad Norm: 2.6485\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.156025\tGrad Norm: 2.0218\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.167324\tGrad Norm: 1.6438\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.082276\tGrad Norm: 1.2834\n",
      "\n",
      "Test set: Average loss: 0.0821, Accuracy: 9736/10000 (97.36%)\n",
      "\n",
      "\n",
      "===== Running AdaGrad with LR=0.01 =====\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301220\tGrad Norm: 1.8177\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.262394\tGrad Norm: 1.8811\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.189327\tGrad Norm: 1.3934\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.167410\tGrad Norm: 1.6455\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.163037\tGrad Norm: 1.1002\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.075617\tGrad Norm: 0.8992\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.150336\tGrad Norm: 1.9201\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.122146\tGrad Norm: 1.5740\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.145337\tGrad Norm: 1.3705\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.117838\tGrad Norm: 1.0047\n",
      "\n",
      "Test set: Average loss: 0.0563, Accuracy: 9814/10000 (98.14%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.248578\tGrad Norm: 1.9680\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.039803\tGrad Norm: 0.9639\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.146480\tGrad Norm: 1.5749\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.061204\tGrad Norm: 0.8665\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.034631\tGrad Norm: 0.4867\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.113522\tGrad Norm: 1.6567\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.269978\tGrad Norm: 2.1384\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.033352\tGrad Norm: 0.8218\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.051755\tGrad Norm: 0.9655\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.112141\tGrad Norm: 1.4725\n",
      "\n",
      "Test set: Average loss: 0.0440, Accuracy: 9851/10000 (98.51%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.047625\tGrad Norm: 1.1902\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.129502\tGrad Norm: 1.3112\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.069939\tGrad Norm: 1.0314\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.043041\tGrad Norm: 0.7510\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.040184\tGrad Norm: 1.1065\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.021627\tGrad Norm: 0.5080\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.069900\tGrad Norm: 0.8487\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.027480\tGrad Norm: 1.0622\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.095931\tGrad Norm: 0.8116\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.056871\tGrad Norm: 0.8383\n",
      "\n",
      "Test set: Average loss: 0.0361, Accuracy: 9876/10000 (98.76%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.060161\tGrad Norm: 1.6512\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.035328\tGrad Norm: 0.7187\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.019305\tGrad Norm: 0.5288\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.120463\tGrad Norm: 1.4811\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.076161\tGrad Norm: 1.7366\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.020897\tGrad Norm: 0.4855\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.049094\tGrad Norm: 1.3071\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.014878\tGrad Norm: 0.3816\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.127634\tGrad Norm: 2.2959\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.063872\tGrad Norm: 1.2959\n",
      "\n",
      "Test set: Average loss: 0.0340, Accuracy: 9879/10000 (98.79%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.023010\tGrad Norm: 0.5898\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.101452\tGrad Norm: 1.0903\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.024426\tGrad Norm: 0.9209\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.018059\tGrad Norm: 0.2946\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.093336\tGrad Norm: 2.2012\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.101454\tGrad Norm: 1.5746\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.038357\tGrad Norm: 0.7907\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.036934\tGrad Norm: 0.9279\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.040709\tGrad Norm: 1.4150\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.083802\tGrad Norm: 1.3346\n",
      "\n",
      "Test set: Average loss: 0.0332, Accuracy: 9894/10000 (98.94%)\n",
      "\n",
      "\n",
      "===== Running AdaGrad with LR=0.001 =====\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301220\tGrad Norm: 1.8177\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.500556\tGrad Norm: 3.4140\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.339505\tGrad Norm: 2.8960\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.330729\tGrad Norm: 3.5374\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.241314\tGrad Norm: 2.1158\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.252082\tGrad Norm: 2.8731\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.325075\tGrad Norm: 2.9975\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.231866\tGrad Norm: 2.5408\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.283895\tGrad Norm: 2.8876\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.287390\tGrad Norm: 2.8547\n",
      "\n",
      "Test set: Average loss: 0.1574, Accuracy: 9531/10000 (95.31%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.422798\tGrad Norm: 3.1322\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.150756\tGrad Norm: 2.0630\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.213089\tGrad Norm: 2.5884\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.224471\tGrad Norm: 2.9118\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.188231\tGrad Norm: 2.4958\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.192332\tGrad Norm: 2.6091\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.365648\tGrad Norm: 3.0281\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.088140\tGrad Norm: 1.7457\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.177364\tGrad Norm: 2.8118\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.255772\tGrad Norm: 2.8979\n",
      "\n",
      "Test set: Average loss: 0.1149, Accuracy: 9644/10000 (96.44%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.168234\tGrad Norm: 2.3350\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.303107\tGrad Norm: 3.6004\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.213194\tGrad Norm: 2.6593\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.141825\tGrad Norm: 2.5160\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.115956\tGrad Norm: 2.0561\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.088606\tGrad Norm: 1.4651\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.244172\tGrad Norm: 2.5106\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.086309\tGrad Norm: 1.7918\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.263510\tGrad Norm: 2.9159\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.236818\tGrad Norm: 3.2525\n",
      "\n",
      "Test set: Average loss: 0.0953, Accuracy: 9707/10000 (97.07%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.070265\tGrad Norm: 1.4625\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.070275\tGrad Norm: 1.4936\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.070768\tGrad Norm: 1.3587\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.140773\tGrad Norm: 1.6790\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.121330\tGrad Norm: 1.9827\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.109919\tGrad Norm: 1.7933\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.066200\tGrad Norm: 1.5827\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.074854\tGrad Norm: 1.7215\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.186995\tGrad Norm: 2.7739\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.164380\tGrad Norm: 2.2633\n",
      "\n",
      "Test set: Average loss: 0.0844, Accuracy: 9738/10000 (97.38%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.060096\tGrad Norm: 1.2664\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.163907\tGrad Norm: 1.9680\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.081275\tGrad Norm: 1.6702\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.083264\tGrad Norm: 1.5151\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.303421\tGrad Norm: 3.0752\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.284790\tGrad Norm: 3.3280\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.151258\tGrad Norm: 2.3412\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.091794\tGrad Norm: 1.7588\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.066195\tGrad Norm: 1.7039\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.105946\tGrad Norm: 2.3720\n",
      "\n",
      "Test set: Average loss: 0.0771, Accuracy: 9762/10000 (97.62%)\n",
      "\n",
      "\n",
      "===== Running RMSProp with LR=0.1 =====\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301220\tGrad Norm: 1.8177\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.262067\tGrad Norm: 0.1138\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.306515\tGrad Norm: 0.1857\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.264548\tGrad Norm: 0.1019\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.315541\tGrad Norm: 0.1204\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.311299\tGrad Norm: 0.1770\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.326194\tGrad Norm: 0.1256\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.336215\tGrad Norm: 0.1634\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.326244\tGrad Norm: 0.1127\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 2.323719\tGrad Norm: 0.1390\n",
      "\n",
      "Test set: Average loss: 2.3141, Accuracy: 1032/10000 (10.32%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.312345\tGrad Norm: 0.0984\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 2.268870\tGrad Norm: 0.1441\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 2.294243\tGrad Norm: 0.0855\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 2.329334\tGrad Norm: 0.1402\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 2.287121\tGrad Norm: 0.0885\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 2.298126\tGrad Norm: 0.1027\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 2.336159\tGrad Norm: 0.1589\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 2.313406\tGrad Norm: 0.1690\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 2.328651\tGrad Norm: 0.1751\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 2.328913\tGrad Norm: 0.1261\n",
      "\n",
      "Test set: Average loss: 2.3157, Accuracy: 974/10000 (9.74%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.288460\tGrad Norm: 0.1155\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 2.318912\tGrad Norm: 0.1263\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 2.319184\tGrad Norm: 0.1154\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 2.316141\tGrad Norm: 0.1224\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 2.319139\tGrad Norm: 0.0697\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 2.298326\tGrad Norm: 0.0907\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 2.304043\tGrad Norm: 0.0942\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 2.290994\tGrad Norm: 0.0544\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 2.308125\tGrad Norm: 0.0946\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 2.282236\tGrad Norm: 0.1172\n",
      "\n",
      "Test set: Average loss: 2.3129, Accuracy: 1028/10000 (10.28%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.334152\tGrad Norm: 0.1135\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 2.328178\tGrad Norm: 0.1438\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 2.319555\tGrad Norm: 0.1202\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 2.309402\tGrad Norm: 0.0892\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 2.323887\tGrad Norm: 0.1365\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 2.304571\tGrad Norm: 0.0923\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 2.394142\tGrad Norm: 0.1795\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 2.308314\tGrad Norm: 0.1809\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 2.335851\tGrad Norm: 0.1667\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 2.312670\tGrad Norm: 0.0963\n",
      "\n",
      "Test set: Average loss: 2.3131, Accuracy: 974/10000 (9.74%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.336410\tGrad Norm: 0.1406\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 2.310070\tGrad Norm: 0.1075\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 2.323510\tGrad Norm: 0.1183\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 2.344960\tGrad Norm: 0.1480\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 2.285672\tGrad Norm: 0.1046\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 2.307625\tGrad Norm: 0.1142\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 2.325426\tGrad Norm: 0.1262\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 2.314901\tGrad Norm: 0.0954\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 2.287898\tGrad Norm: 0.0952\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 2.344560\tGrad Norm: 0.1712\n",
      "\n",
      "Test set: Average loss: 2.3164, Accuracy: 1135/10000 (11.35%)\n",
      "\n",
      "\n",
      "===== Running RMSProp with LR=0.01 =====\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301220\tGrad Norm: 1.8177\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.258124\tGrad Norm: 1.0561\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.096438\tGrad Norm: 3.0954\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.532992\tGrad Norm: 3.7728\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.583424\tGrad Norm: 2.5915\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.643190\tGrad Norm: 1.8352\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.569238\tGrad Norm: 1.9754\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.287108\tGrad Norm: 1.4095\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.439884\tGrad Norm: 2.2483\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.605774\tGrad Norm: 2.6985\n",
      "\n",
      "Test set: Average loss: 0.2573, Accuracy: 9262/10000 (92.62%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.455894\tGrad Norm: 1.4756\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.306757\tGrad Norm: 1.5072\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.237818\tGrad Norm: 1.5188\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.391715\tGrad Norm: 1.2691\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.333273\tGrad Norm: 1.5937\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.363349\tGrad Norm: 1.3471\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.565382\tGrad Norm: 1.2862\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.363826\tGrad Norm: 1.1987\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.224724\tGrad Norm: 1.0525\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.521826\tGrad Norm: 1.7413\n",
      "\n",
      "Test set: Average loss: 0.1931, Accuracy: 9458/10000 (94.58%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.404652\tGrad Norm: 2.4051\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.472858\tGrad Norm: 2.9568\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.301139\tGrad Norm: 0.9629\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.203544\tGrad Norm: 1.2048\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.273968\tGrad Norm: 1.7432\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.189815\tGrad Norm: 1.0979\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.472418\tGrad Norm: 1.9245\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.313437\tGrad Norm: 1.8984\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.355007\tGrad Norm: 1.2069\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.387269\tGrad Norm: 2.3019\n",
      "\n",
      "Test set: Average loss: 0.1260, Accuracy: 9632/10000 (96.32%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.184078\tGrad Norm: 1.4464\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.127766\tGrad Norm: 0.7788\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.172536\tGrad Norm: 1.2639\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.231166\tGrad Norm: 1.4541\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.148839\tGrad Norm: 0.9420\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.266820\tGrad Norm: 1.8356\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.161989\tGrad Norm: 1.6827\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.087167\tGrad Norm: 0.6951\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.262559\tGrad Norm: 2.3605\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.329464\tGrad Norm: 2.6860\n",
      "\n",
      "Test set: Average loss: 0.1231, Accuracy: 9626/10000 (96.26%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.175401\tGrad Norm: 1.5427\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.294449\tGrad Norm: 2.1413\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.178526\tGrad Norm: 1.7449\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.120067\tGrad Norm: 1.0162\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.432490\tGrad Norm: 2.2172\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.371266\tGrad Norm: 3.1363\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.157377\tGrad Norm: 1.1253\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.174797\tGrad Norm: 1.0305\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.080883\tGrad Norm: 0.7209\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.030911\tGrad Norm: 0.6334\n",
      "\n",
      "Test set: Average loss: 0.0891, Accuracy: 9735/10000 (97.35%)\n",
      "\n",
      "\n",
      "===== Running RMSProp with LR=0.001 =====\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301220\tGrad Norm: 1.8177\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.257976\tGrad Norm: 1.7953\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.174038\tGrad Norm: 1.3908\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.166571\tGrad Norm: 1.6583\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.126008\tGrad Norm: 1.0369\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.070193\tGrad Norm: 1.0896\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.120705\tGrad Norm: 1.4938\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.105640\tGrad Norm: 1.5897\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.116825\tGrad Norm: 1.3467\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.183245\tGrad Norm: 1.2963\n",
      "\n",
      "Test set: Average loss: 0.0498, Accuracy: 9837/10000 (98.37%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.120690\tGrad Norm: 1.5119\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.024182\tGrad Norm: 0.6658\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.143028\tGrad Norm: 1.9085\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.033051\tGrad Norm: 0.5632\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.052639\tGrad Norm: 0.8551\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.137673\tGrad Norm: 1.8105\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.232364\tGrad Norm: 2.0913\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.014935\tGrad Norm: 0.5085\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.102585\tGrad Norm: 1.0515\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.089217\tGrad Norm: 0.9217\n",
      "\n",
      "Test set: Average loss: 0.0389, Accuracy: 9868/10000 (98.68%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.053759\tGrad Norm: 1.0541\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.162572\tGrad Norm: 1.4692\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.067970\tGrad Norm: 0.9914\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.026703\tGrad Norm: 0.4644\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.031456\tGrad Norm: 0.7361\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.005087\tGrad Norm: 0.1358\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.077663\tGrad Norm: 0.9129\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.016420\tGrad Norm: 0.6916\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.074750\tGrad Norm: 0.9595\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.033271\tGrad Norm: 0.6690\n",
      "\n",
      "Test set: Average loss: 0.0329, Accuracy: 9893/10000 (98.93%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.102989\tGrad Norm: 1.5295\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.051207\tGrad Norm: 0.9987\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.030614\tGrad Norm: 0.6101\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.126993\tGrad Norm: 1.4580\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.024817\tGrad Norm: 0.9953\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.040388\tGrad Norm: 0.9733\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.030587\tGrad Norm: 0.9532\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.006727\tGrad Norm: 0.2001\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.094159\tGrad Norm: 1.1784\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.063041\tGrad Norm: 0.7587\n",
      "\n",
      "Test set: Average loss: 0.0309, Accuracy: 9901/10000 (99.01%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.007171\tGrad Norm: 0.1648\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.084599\tGrad Norm: 0.8702\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.005258\tGrad Norm: 0.1375\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.009030\tGrad Norm: 0.2117\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.095429\tGrad Norm: 1.8244\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.112689\tGrad Norm: 1.3735\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.027985\tGrad Norm: 0.5405\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.054655\tGrad Norm: 1.5987\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.023695\tGrad Norm: 0.9890\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.029413\tGrad Norm: 0.9567\n",
      "\n",
      "Test set: Average loss: 0.0390, Accuracy: 9888/10000 (98.88%)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-05T14:22:52.672666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from model.CNN import CNNnet\n",
    "from model.train import train\n",
    "from model.test import test\n",
    "from utils.data_utils import get_mnist_loaders\n",
    "from utils.hparams import get_hyperparams\n",
    "\n",
    "# 学习率列表\n",
    "learning_rates = [0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001]\n",
    "\n",
    "# 创建保存文件夹\n",
    "os.makedirs(\"results/adam_lr_experiments\", exist_ok=True)\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\n===== Running Adam with LR={lr} =====\")\n",
    "    args = get_hyperparams(optimizer='Adam', lr=lr)\n",
    "    train_loader, test_loader, device = get_mnist_loaders(\n",
    "        batch_size=args.batch_size,\n",
    "        test_batch_size=args.test_batch_size,\n",
    "        seed=args.seed,\n",
    "        use_cuda=args.use_cuda,\n",
    "        shuffle=args.shuffle,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=args.pin_memory\n",
    "    )\n",
    "\n",
    "    model = CNNnet().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    avg_grad_norms = []\n",
    "    test_accuracies = []\n",
    "    grad_norms_per_epoch = []\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        grad_norms = train(model, device, train_loader, optimizer, epoch, args.log_interval)\n",
    "        avg_grad_norms.append(sum(grad_norms) / len(grad_norms))\n",
    "        grad_norms_per_epoch.append(grad_norms)\n",
    "        _, acc = test(model, device, test_loader)\n",
    "        test_accuracies.append(acc)\n",
    "\n",
    "    # 绘图\n",
    "    plt.figure(figsize=(15, 4))\n",
    "\n",
    "    # 1. 每轮平均梯度范数\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(range(1, args.epochs + 1), avg_grad_norms, marker='o')\n",
    "    plt.title(f\"Avg Gradient Norm per Epoch (lr={lr})\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"L2 Norm\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 2. 测试准确率\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(range(1, args.epochs + 1), test_accuracies, marker='x', color='green')\n",
    "    plt.title(f\"Test Accuracy per Epoch (lr={lr})\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 3. 最后一轮每 batch 的梯度范数\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(grad_norms_per_epoch[-1], marker='.')\n",
    "    plt.title(f\"Gradient Norm per Batch (Last Epoch, lr={lr})\")\n",
    "    plt.xlabel(\"Batch\")\n",
    "    plt.ylabel(\"L2 Norm\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = f\"results/adam_lr_experiments/adam_lr_{str(lr).replace('.', '_')}.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n"
   ],
   "id": "d9bc47df5f538bbd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Running Adam with LR=0.1 =====\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301220\tGrad Norm: 1.8177\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.311620\tGrad Norm: 0.1538\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.295421\tGrad Norm: 0.0678\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.308861\tGrad Norm: 0.1372\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.324122\tGrad Norm: 0.1248\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.286736\tGrad Norm: 0.1616\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.307297\tGrad Norm: 0.1088\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.291949\tGrad Norm: 0.1351\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.314351\tGrad Norm: 0.1027\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 2.332623\tGrad Norm: 0.1488\n",
      "\n",
      "Test set: Average loss: 2.3076, Accuracy: 1010/10000 (10.10%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.290286\tGrad Norm: 0.0697\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 2.255116\tGrad Norm: 0.1320\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 2.284819\tGrad Norm: 0.0740\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "dad0fe1201f85a18"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pytorch]",
   "language": "python",
   "name": "conda-env-.conda-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
